{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9HmqQvjrzWJJ6t6+J2rif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Search-Prem/E-Waste-Generation-Classification/blob/main/E_Waste_Generation_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f09db4"
      },
      "source": [
        "# E-Waste Classification with EfficientNetV2B0\n",
        "\n",
        "This notebook trains a convolutional neural network (CNN) using the EfficientNetV2B0 architecture to classify images of e-waste into 10 categories. It includes steps for data loading, preprocessing, model building, training, evaluation, and creating a Gradio interface for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e267e10c"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "This cell installs the necessary libraries and imports them for use in the notebook.\n",
        "- `tensorflow`: The main deep learning library.\n",
        "- `scikit-learn`: Used for calculating class weights.\n",
        "- `matplotlib`: Used for plotting the training history.\n",
        "- `gradio`: Used to create a web interface for the model.\n",
        "- `PIL`: Used for image manipulation within the Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c49d04ba",
        "outputId": "98772945-d11f-4354-b3e9-eded78516ecc"
      },
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow scikit-learn matplotlib gradio\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gradio as gr\n",
        "from PIL import Image"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65af1525"
      },
      "source": [
        "## Model Definition and Compilation\n",
        "\n",
        "This section defines the CNN model. It uses a pre-trained EfficientNetV2B0 model as the base, adds data augmentation layers, global average pooling, a dense layer, dropout, and a final dense layer for classification. The base model's first 100 layers are frozen during initial training. The model is compiled with the Adam optimizer, sparse categorical crossentropy loss, and accuracy as the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53fdea01",
        "outputId": "7b6d6606-d9f3-4df7-dbfd-dfd7000b7a52"
      },
      "source": [
        "# Enhanced data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.15),\n",
        "    layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "# Load pre-trained model\n",
        "base_model = EfficientNetV2B0(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze first 100 layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build model with extra dense layer - Removed data_augmentation here\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(128, 128, 3)),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),  # New layer added\n",
        "    layers.Dropout(0.3),                  # Increased dropout\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with lower initial learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9f7260"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "This section defines the paths to the training, validation, and test datasets and loads them using `tf.keras.utils.image_dataset_from_directory`. It also calculates class weights to handle potential class imbalance in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7c32cf0",
        "outputId": "3ade3289-90ae-48ec-e7ef-92d64e39950c"
      },
      "source": [
        "# Set paths to your dataset - Update these paths based on where your data is in Colab\n",
        "# For example, if your data is in Google Drive, you might use:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# train_path = '/content/drive/My Drive/E waste data/modified-dataset/train'\n",
        "# test_path = '/content/drive/My Drive/E waste data/modified-dataset/test'\n",
        "# val_path = '/content/drive/My Drive/E waste data/modified-dataset/val'\n",
        "# Or if you've uploaded your data directly to the Colab environment:\n",
        "# train_path = '/content/modified-dataset/train'\n",
        "# test_path = '/content/modified-dataset/test'\n",
        "# val_path = '/content/modified-dataset/val'\n",
        "\n",
        "# Replace the following lines with the correct paths for your Colab environment\n",
        "# Assuming the zip file extracted to a directory named 'modified-dataset' in /content/\n",
        "train_path = '/content/modified-dataset/train'\n",
        "test_path = '/content/modified-dataset/test'\n",
        "val_path = '/content/modified-dataset/val'\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_path,\n",
        "    shuffle=True,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_path,\n",
        "    shuffle=True,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_path,\n",
        "    shuffle=False,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Get class names\n",
        "class_names = train_data.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "y_train = np.concatenate([y.numpy() for _, y in train_data], axis=0)\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2400 files belonging to 10 classes.\n",
            "Found 300 files belonging to 10 classes.\n",
            "Found 300 files belonging to 10 classes.\n",
            "Class names: ['Battery', 'Keyboard', 'Microwave', 'Mobile', 'Mouse', 'PCB', 'Player', 'Printer', 'Television', 'Washing Machine']\n",
            "Class weights: {0: np.float64(1.0), 1: np.float64(1.0), 2: np.float64(1.0), 3: np.float64(1.0), 4: np.float64(1.0), 5: np.float64(1.0), 6: np.float64(1.0), 7: np.float64(1.0), 8: np.float64(1.0), 9: np.float64(1.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63060061"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "This section defines callbacks for early stopping and learning rate reduction, and then trains the model using the training data and validation data. The class weights are used to address the imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ba5dc5",
        "outputId": "88bb8317-e729-4e51-f2f3-5c81d68510df"
      },
      "source": [
        "# Callbacks\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train for more epochs (early stopping will prevent overtraining)\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=30,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 329ms/step - accuracy: 0.1920 - loss: 2.2305 - val_accuracy: 0.6633 - val_loss: 1.4916 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 161ms/step - accuracy: 0.6232 - loss: 1.4001 - val_accuracy: 0.8933 - val_loss: 0.6815 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 162ms/step - accuracy: 0.7884 - loss: 0.7820 - val_accuracy: 0.9167 - val_loss: 0.4048 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.8477 - loss: 0.5168 - val_accuracy: 0.9367 - val_loss: 0.2818 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.8861 - loss: 0.3992 - val_accuracy: 0.9500 - val_loss: 0.2313 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.8865 - loss: 0.3565 - val_accuracy: 0.9500 - val_loss: 0.1984 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.9067 - loss: 0.3044 - val_accuracy: 0.9433 - val_loss: 0.1985 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.9266 - loss: 0.2584 - val_accuracy: 0.9500 - val_loss: 0.1900 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 163ms/step - accuracy: 0.9295 - loss: 0.2331 - val_accuracy: 0.9367 - val_loss: 0.1887 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.9386 - loss: 0.1998 - val_accuracy: 0.9433 - val_loss: 0.1819 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.9481 - loss: 0.1622 - val_accuracy: 0.9367 - val_loss: 0.1781 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.9492 - loss: 0.1565 - val_accuracy: 0.9433 - val_loss: 0.1702 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.9593 - loss: 0.1301 - val_accuracy: 0.9500 - val_loss: 0.1725 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.9592 - loss: 0.1170 - val_accuracy: 0.9500 - val_loss: 0.1757 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.9565 - loss: 0.1370 - val_accuracy: 0.9500 - val_loss: 0.1718 - learning_rate: 5.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b906038"
      },
      "source": [
        "## Evaluation and Visualization\n",
        "\n",
        "This section evaluates the trained model on the test set and prints the test accuracy and loss. It also plots the training and validation accuracy and loss over epochs to visualize the training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c939231e"
      },
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Over Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f521b163"
      },
      "source": [
        "## Model Saving and Gradio Interface\n",
        "\n",
        "This section saves the trained model to a file. It then defines a Gradio interface for classifying new images. The `classify_image` function preprocesses the input image, makes a prediction using the trained model, and returns the predicted class and confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "247923a8"
      },
      "source": [
        "# Save model\n",
        "model.save('efficientnet_ewaste_classifier.keras')\n",
        "\n",
        "# Gradio interface\n",
        "def classify_image(img):\n",
        "    img = img.resize((128, 128))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = class_names[np.argmax(prediction)]\n",
        "    confidence = np.max(prediction)\n",
        "\n",
        "    return f\"Predicted: {predicted_class} (Confidence: {confidence:.2f})\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=classify_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    examples=[[\"sample_battery.jpg\"], [\"sample_keyboard.jpg\"]],\n",
        "    title=\"E-Waste Classifier\",\n",
        "    description=\"Upload an image of e-waste to classify it into one of 10 categories.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}