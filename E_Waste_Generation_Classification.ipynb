{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9HmqQvjrzWJJ6t6+J2rif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Search-Prem/E-Waste-Generation-Classification/blob/main/E_Waste_Generation_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f09db4"
      },
      "source": [
        "# E-Waste Classification with EfficientNetV2B0\n",
        "\n",
        "This notebook trains a convolutional neural network (CNN) using the EfficientNetV2B0 architecture to classify images of e-waste into 10 categories. It includes steps for data loading, preprocessing, model building, training, evaluation, and creating a Gradio interface for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e267e10c"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "This cell installs the necessary libraries and imports them for use in the notebook.\n",
        "- `tensorflow`: The main deep learning library.\n",
        "- `scikit-learn`: Used for calculating class weights.\n",
        "- `matplotlib`: Used for plotting the training history.\n",
        "- `gradio`: Used to create a web interface for the model.\n",
        "- `PIL`: Used for image manipulation within the Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49d04ba"
      },
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow scikit-learn matplotlib gradio\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gradio as gr\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65af1525"
      },
      "source": [
        "## Model Definition and Compilation\n",
        "\n",
        "This section defines the CNN model. It uses a pre-trained EfficientNetV2B0 model as the base, adds data augmentation layers, global average pooling, a dense layer, dropout, and a final dense layer for classification. The base model's first 100 layers are frozen during initial training. The model is compiled with the Adam optimizer, sparse categorical crossentropy loss, and accuracy as the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53fdea01"
      },
      "source": [
        "# Enhanced data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.15),\n",
        "    layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "# Load pre-trained model\n",
        "base_model = EfficientNetV2B0(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze first 100 layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build model with extra dense layer - Removed data_augmentation here\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(128, 128, 3)),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),  # New layer added\n",
        "    layers.Dropout(0.3),                  # Increased dropout\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with lower initial learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9f7260"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "This section defines the paths to the training, validation, and test datasets and loads them using `tf.keras.utils.image_dataset_from_directory`. It also calculates class weights to handle potential class imbalance in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7c32cf0"
      },
      "source": [
        "# Set paths to your dataset - Update these paths based on where your data is in Colab\n",
        "# For example, if your data is in Google Drive, you might use:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# train_path = '/content/drive/My Drive/E waste data/modified-dataset/train'\n",
        "# test_path = '/content/drive/My Drive/E waste data/modified-dataset/test'\n",
        "# val_path = '/content/drive/My Drive/E waste data/modified-dataset/val'\n",
        "# Or if you've uploaded your data directly to the Colab environment:\n",
        "# train_path = '/content/modified-dataset/train'\n",
        "# test_path = '/content/modified-dataset/test'\n",
        "# val_path = '/content/modified-dataset/val'\n",
        "\n",
        "# Replace the following lines with the correct paths for your Colab environment\n",
        "# Assuming the zip file extracted to a directory named 'modified-dataset' in /content/\n",
        "train_path = '/content/modified-dataset/train'\n",
        "test_path = '/content/modified-dataset/test'\n",
        "val_path = '/content/modified-dataset/val'\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_path,\n",
        "    shuffle=True,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_path,\n",
        "    shuffle=True,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_path,\n",
        "    shuffle=False,\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Get class names\n",
        "class_names = train_data.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "y_train = np.concatenate([y.numpy() for _, y in train_data], axis=0)\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63060061"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "This section defines callbacks for early stopping and learning rate reduction, and then trains the model using the training data and validation data. The class weights are used to address the imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ba5dc5"
      },
      "source": [
        "# Callbacks\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train for more epochs (early stopping will prevent overtraining)\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=30,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b906038"
      },
      "source": [
        "## Evaluation and Visualization\n",
        "\n",
        "This section evaluates the trained model on the test set and prints the test accuracy and loss. It also plots the training and validation accuracy and loss over epochs to visualize the training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c939231e"
      },
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Over Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f521b163"
      },
      "source": [
        "## Model Saving and Gradio Interface\n",
        "\n",
        "This section saves the trained model to a file. It then defines a Gradio interface for classifying new images. The `classify_image` function preprocesses the input image, makes a prediction using the trained model, and returns the predicted class and confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "247923a8"
      },
      "source": [
        "# Save model\n",
        "model.save('efficientnet_ewaste_classifier.keras')\n",
        "\n",
        "# Gradio interface\n",
        "def classify_image(img):\n",
        "    img = img.resize((128, 128))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = class_names[np.argmax(prediction)]\n",
        "    confidence = np.max(prediction)\n",
        "\n",
        "    return f\"Predicted: {predicted_class} (Confidence: {confidence:.2f})\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=classify_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    examples=[[\"sample_battery.jpg\"], [\"sample_keyboard.jpg\"]],\n",
        "    title=\"E-Waste Classifier\",\n",
        "    description=\"Upload an image of e-waste to classify it into one of 10 categories.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}